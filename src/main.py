"""
MyAssistent API - –ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π —Ç–æ—Ä–≥–æ–≤—ã–π –±–æ—Ç —Å ML –¥–ª—è Bybit
–í–µ—Ä—Å–∏—è 0.8 - –ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å —Ä–æ—É—Ç–µ—Ä–∞–º–∏
"""
from __future__ import annotations

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞ (–î–û–õ–ñ–ù–û –ë–´–¢–¨ –ü–ï–†–í–´–ú!)
from dotenv import load_dotenv
load_dotenv()

import os
import json
from pathlib import Path
from fastapi import Depends
from fastapi.responses import RedirectResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.triggers.interval import IntervalTrigger
from sqlalchemy.orm import Session

# –ò–º–ø–æ—Ä—Ç—ã —Ä–æ—É—Ç–µ—Ä–æ–≤
from src.routers import (
    news,
    prices,
    dataset,
    report,
    watchlist,
    risk,
    notify,
    models,
    signals,
    trade,
    automation,
    ui,
    journal,
    backup,
    db_admin,
    debug,
    backtest,
    rl,
    mlflow_registry,
)

# –ò–º–ø–æ—Ä—Ç—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏ —É—Ç–∏–ª–∏—Ç
from src.dependencies import get_db, require_api_key
from src.utils import _now_utc
from src.db import SessionLocal, Message

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è job —Ñ—É–Ω–∫—Ü–∏–π
from src.news import fetch_and_store
from src.analysis import analyze_new_articles
from src.prices import fetch_and_store_prices
from src.features import build_dataset
from src.reports import build_daily_report
from src.watchlist import pairs_for_jobs, discover_pairs
from src.modeling import train_xgb_and_save
from src.model_policy import load_model_policy
from src.model_registry import get_active_model_path, set_active_model
from src.risk import load_policy


# ============== –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ==============

API_KEY = (os.getenv("API_KEY") or "").strip()
print(f"[config] API_KEY loaded: {bool(API_KEY)}")

USE_OFFLINE = os.getenv("OFFLINE_DOCS", "1") == "1"
ENABLE_DOCS = os.getenv("ENABLE_DOCS", "1") == "1"

try:
    if USE_OFFLINE:
        from fastapi_offline import FastAPIOffline as _FastAPI
    else:
        from fastapi import FastAPI as _FastAPI
except Exception:
    from fastapi import FastAPI as _FastAPI
    USE_OFFLINE = False


# ============== FastAPI App ==============

app = _FastAPI(
    title="MyAssistent API",
    version="0.9",
    docs_url="/docs" if ENABLE_DOCS else None,
    redoc_url="/redoc" if ENABLE_DOCS else None,
    swagger_ui_parameters={"persistAuthorization": True, "displayRequestDuration": True},
    description=(
        "Backend for signals, paper-trading and news radar.\n\n"
        "Auth: send X-API-Key for all endpoints, –∫—Ä–æ–º–µ /, /ping –∏ HTML-–ø–∞–Ω–µ–ª–µ–π.\n"
        "Responses: –≤—Å–µ —Ä—É—á–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç {'status':'ok', ...}. –°–ø–∏—Å–∫–∏ ‚Äî –≤ –ø–æ–ª–µ 'data'.\n"
        "–û—à–∏–±–∫–∏: raise HTTPException —Å detail={'status':'error','code':..., 'detail':...}."
    ),
)

# ============== Prometheus Metrics ==============

METRICS_ENABLED = False
try:
    from prometheus_fastapi_instrumentator import Instrumentator
    
    # –í–∫–ª—é—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–º–æ–∂–Ω–æ –æ—Ç–∫–ª—é—á–∏—Ç—å —á–µ—Ä–µ–∑ ENABLE_METRICS=false)
    enable_metrics = os.getenv("ENABLE_METRICS", "true").lower() not in ("false", "0", "no")
    
    if enable_metrics:
        instrumentator = Instrumentator(
            should_group_status_codes=False,
            should_ignore_untemplated=True,
            should_respect_env_var=False,
            should_instrument_requests_inprogress=True,
            excluded_handlers=["/metrics"],
            inprogress_name="fastapi_inprogress",
            inprogress_labels=True,
        )
        
        instrumentator.instrument(app).expose(app, endpoint="/metrics", include_in_schema=True)
        METRICS_ENABLED = True
        print("[metrics] Prometheus metrics enabled at /metrics")
    else:
        print("[metrics] Metrics disabled via ENABLE_METRICS=false")
except ImportError:
    print("[metrics] prometheus-fastapi-instrumentator not installed, metrics disabled")
except Exception as e:
    print(f"[metrics] Error enabling Prometheus metrics: {e}")


# ============== CORS Middleware ==============

CORS_ORIGINS = [o.strip() for o in os.getenv("CORS_ORIGINS", "*").split(",") if o.strip()]
allow_all = CORS_ORIGINS == ["*"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"] if allow_all else CORS_ORIGINS,
    allow_credentials=False if allow_all else True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============== Static Files (artifacts) ==============

Path("artifacts").mkdir(exist_ok=True)
if os.getenv("PUBLIC_ARTIFACTS", "0") == "1":
    app.mount("/artifacts", StaticFiles(directory="artifacts"), name="artifacts")
else:
    @app.get("/artifacts/{path:path}", tags=["Files"])
    def artifacts_secure(path: str, _=Depends(require_api_key)):
        from fastapi import HTTPException
        full = (Path("artifacts") / path).resolve()
        root = Path("artifacts").resolve()
        if root not in full.parents and full != root:
            raise HTTPException(404)
        if not full.is_file() or not str(full).startswith(str(root)):
            raise HTTPException(404)
        return FileResponse(str(full))


# ============== –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –†–æ—É—Ç–µ—Ä–æ–≤ ==============

app.include_router(news.router)
app.include_router(prices.router)
app.include_router(dataset.router)
app.include_router(report.router)
app.include_router(watchlist.router)
app.include_router(risk.router)
app.include_router(notify.router)
app.include_router(models.router)
app.include_router(signals.router)
app.include_router(trade.router)
app.include_router(automation.router)
app.include_router(ui.router)
app.include_router(journal.router)
app.include_router(backup.router)
app.include_router(db_admin.router)
app.include_router(debug.router)
app.include_router(backtest.router)
app.include_router(rl.router)
app.include_router(mlflow_registry.router)


# ============== –ö–æ—Ä–Ω–µ–≤—ã–µ –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã ==============

@app.get("/")
def root():
    """–†–µ–¥–∏—Ä–µ–∫—Ç –Ω–∞ /docs –∏–ª–∏ /ping"""
    return RedirectResponse("/docs" if ENABLE_DOCS else "/ping")


@app.get("/ping")
def ping():
    """Health check"""
    return {"pong": True}


@app.get("/hello", tags=["Memory"])
def say_hello(name: str = "–ù–∏–∫–∏—Ç–∞"):
    """–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ"""
    return {"message": f"–ü—Ä–∏–≤–µ—Ç, {name}! üöÄ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç."}


@app.get("/time", tags=["Memory"])
def get_time():
    """–¢–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è —Å–µ—Ä–≤–µ—Ä–∞"""
    return {"time": _now_utc().strftime("%Y-%m-%d %H:%M:%S %Z")}


@app.post("/memory/add", tags=["Memory"])
def add_message(text: str, db: Session = Depends(get_db), _=Depends(require_api_key)):
    """–î–æ–±–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –ø–∞–º—è—Ç—å"""
    msg = Message(text=text)
    db.add(msg)
    db.commit()
    db.refresh(msg)
    return {"id": msg.id, "text": msg.text, "created_at": msg.created_at}


@app.get("/memory/add", tags=["Memory"])
def add_message_get(text: str, db: Session = Depends(get_db), _=Depends(require_api_key)):
    """–î–æ–±–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –ø–∞–º—è—Ç—å (GET)"""
    msg = Message(text=text)
    db.add(msg)
    db.commit()
    db.refresh(msg)
    return {"id": msg.id, "text": msg.text, "created_at": msg.created_at}


@app.get("/memory/all", tags=["Memory"])
def get_messages(db: Session = Depends(get_db), _=Depends(require_api_key)):
    """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ –ø–∞–º—è—Ç–∏"""
    msgs = db.query(Message).order_by(Message.id.desc()).all()
    return [{"id": m.id, "text": m.text, "created_at": m.created_at} for m in msgs]


# ============== APScheduler Jobs ==============

scheduler = BackgroundScheduler(timezone="UTC")


def job_build_report():
    """–ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π –æ—Ç—á—ë—Ç"""
    with SessionLocal() as db:
        try:
            pairs = [
                ("bybit", "BTC/USDT", "15m"),
                ("bybit", "ETH/USDT", "15m"),
            ]
            path = build_daily_report(db, pairs)
            print(f"[scheduler] report built: {path}")
        except Exception as e:
            print(f"[scheduler] report error: {e}")


def job_fetch_news():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π (RSS)"""
    with SessionLocal() as db:
        try:
            added = fetch_and_store(db)
            print(f"[scheduler] news fetched: +{added}")
        except Exception as e:
            print(f"[scheduler] news fetch error: {e}")


def job_analyze_news():
    """–ê–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π (sentiment + tags)"""
    with SessionLocal() as db:
        try:
            processed = analyze_new_articles(db, limit=200)
            print(f"[scheduler] news analyzed: {processed}")
        except Exception as e:
            print(f"[scheduler] news analyze error: {e}")


def job_discover_watchlist():
    """Auto-discovery —Ç–æ–ø-–ø–∞—Ä –ø–æ –æ–±—ä—ë–º—É"""
    try:
        res = discover_pairs(
            min_volume_usd=2_000_000,
            top_n_per_exchange=25,
            quotes=("USDT",),
            timeframes=("15m",),
            limit=1000,
            exchanges=("bybit",),
        )
        print(f"[scheduler] discover_watchlist: +{len(res.get('added', []))} (watchlist={res.get('total_watchlist')})")
    except Exception as e:
        print(f"[scheduler] discover_watchlist error: {e}")


def job_fetch_prices():
    """–ó–∞–≥—Ä—É–∑–∫–∞ OHLCV –¥–ª—è watchlist"""
    with SessionLocal() as db:
        pairs = pairs_for_jobs()
        for ex, sym, tf, lim in pairs:
            if ex != "bybit":
                continue
            try:
                added = fetch_and_store_prices(db, ex, sym, tf, lim)
                print(f"[scheduler] prices {ex} {sym} {tf}: +{added}")
            except Exception as e:
                print(f"[scheduler] prices error {ex} {sym} {tf}: {e}")


def job_train_models():
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ SLA (–Ω–æ—á—å—é)"""
    policy = load_model_policy()
    with SessionLocal() as db:
        pairs = pairs_for_jobs()
        for ex, sym, tf, _ in pairs:
            try:
                from src.db import ModelRun
                hz = 6 if tf.endswith("h") else 12
                df, feature_cols = build_dataset(db, ex, sym, tf, hz)
                if len(df) < int(policy.get("min_train_rows", 200)):
                    print(f"[scheduler] skip train {ex} {sym} {tf}: not enough data ({len(df)})")
                    continue

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
                need, reason, meta = _model_needs_retrain(db, ex, sym, tf, hz, policy, df_len=len(df))
                if not need:
                    print(f"[scheduler] fresh model {ex} {sym} {tf} ({reason}) ‚Äî skip")
                    continue

                metrics, model_path = train_xgb_and_save(df, feature_cols, artifacts_dir="artifacts")
                run = ModelRun(
                    exchange=ex,
                    symbol=sym,
                    timeframe=tf,
                    horizon_steps=hz,
                    n_train=metrics["n_train"],
                    n_test=metrics["n_test"],
                    accuracy=metrics.get("accuracy"),
                    roc_auc=metrics.get("roc_auc"),
                    threshold=metrics.get("threshold"),
                    total_return=metrics.get("total_return"),
                    sharpe_like=metrics.get("sharpe_like"),
                    model_path=model_path,
                    features_json=json.dumps({"features": feature_cols}, ensure_ascii=False),
                )
                db.add(run)
                db.commit()

                if not get_active_model_path(ex, sym, tf, hz):
                    set_active_model(ex, sym, tf, hz, model_path)

                print(f"[scheduler] trained {ex} {sym} {tf}: AUC={metrics.get('roc_auc'):.3f}")
            except Exception as e:
                print(f"[scheduler] train error {ex} {sym} {tf}: {e}")


def job_make_signals():
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤ –∫–∞–∂–¥—ã–µ 15 –º–∏–Ω—É—Ç"""
    from src.routers.signals import _compute_signal_for_last_bar
    from src.db import SignalEvent
    from src.notify import maybe_send_signal_notification
    
    with SessionLocal() as db:
        pairs = pairs_for_jobs()
        for ex, sym, tf, _ in pairs:
            try:
                hz = 6 if tf.endswith("h") else 12
                result = _compute_signal_for_last_bar(db, ex, sym, tf, hz, None)
                
                if result.get("status") == "ok":
                    signal = result.get("signal")
                    if signal == "buy":
                        print(f"[scheduler] signal BUY {ex} {sym} {tf}: prob={result.get('prob_up', 0):.3f}")
                        
                        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
                        evt = SignalEvent(
                            exchange=ex,
                            symbol=sym,
                            timeframe=tf,
                            horizon_steps=hz,
                            bar_dt=result.get("bar_dt"),
                            close=result.get("close"),
                            prob_up=result.get("prob_up"),
                            threshold=result.get("threshold"),
                            signal=signal,
                            model_path=result.get("model_path"),
                            note=json.dumps({
                                "prob": result.get("prob_up"),
                                "threshold": result.get("threshold"),
                                "prob_gap": result.get("prob_gap"),
                                "metrics": result.get("metrics", {}),
                                "reasons": result.get("reasons", []),
                            }, ensure_ascii=False),
                        )
                        db.add(evt)
                        try:
                            db.commit()
                            # –û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –≤ Telegram
                            maybe_send_signal_notification(
                                signal,
                                result.get("prob_up"),
                                result.get("threshold"),
                                result.get("prob_gap"),
                                result.get("reasons", []),
                                result.get("model_path"),
                                ex, sym, tf,
                                result.get("bar_dt"),
                                result.get("close"),
                                source="scheduler",
                            )
                        except Exception:
                            db.rollback()
            except Exception as e:
                print(f"[scheduler] signal error {ex} {sym} {tf}: {e}")


def job_resolve_outcomes():
    """–†–µ–∑–æ–ª–≤ –∏—Å—Ö–æ–¥–æ–≤ —Å–∏–≥–Ω–∞–ª–æ–≤"""
    from src.db import SignalEvent, SignalOutcome
    with SessionLocal() as db:
        try:
            events = db.query(SignalEvent).filter(SignalEvent.signal.in_(["buy", "sell"])).order_by(SignalEvent.id.desc()).limit(100).all()
            resolved = 0
            for evt in events:
                existing = db.query(SignalOutcome).filter(SignalOutcome.signal_event_id == evt.id).first()
                if existing:
                    continue
                # –ü—ã—Ç–∞–µ–º—Å—è —Ä–µ–∑–æ–ª–≤–∏—Ç—å (—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞)
                # TODO: –ø–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –≤ –±—É–¥—É—â–µ–º
                resolved += 1
            if resolved > 0:
                print(f"[scheduler] resolve_outcomes: {resolved}")
        except Exception as e:
            print(f"[scheduler] resolve_outcomes error: {e}")


def job_monitor_positions():
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–æ–∑–∏—Ü–∏–π (partial close, flat)"""
    # TODO: –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –ª–æ–≥–∏–∫—É –∏–∑ main_old.py
    pass


def job_bootstrap():
    """–ù–∞—á–∞–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ"""
    print("[scheduler] bootstrap: indexes ensured")


def job_self_audit_and_notify():
    """–°–∞–º–æ–∞—É–¥–∏—Ç –∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –≤ Telegram"""
    # TODO: –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –ª–æ–≥–∏–∫—É –∏–∑ main_old.py
    pass


def job_news_radar():
    """News Radar - –¥–µ—Ç–µ–∫—Ç–æ—Ä –≤—Å–ø–ª–µ—Å–∫–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π"""
    policy = load_policy()
    cfg = policy.get("news_radar") or {}
    if not bool(cfg.get("enabled", False)):
        return
    
    from src.routers.news import news_radar, NewsRadarRequest
    window_minutes = int(cfg.get("window_minutes", 90))
    lookback_windows = int(cfg.get("lookback_windows", 6))
    symbols = cfg.get("symbols") or ["BTC", "ETH", "SOL", "XRP"]
    
    with SessionLocal() as db:
        try:
            res = news_radar(
                NewsRadarRequest(
                    window_minutes=window_minutes,
                    lookback_windows=lookback_windows,
                    symbols=symbols,
                    notify=True
                ),
                db,
            )
            if res.get("status") == "ok" and res.get("spike"):
                print("[scheduler] news_radar: SPIKE detected")
        except Exception as e:
            print(f"[scheduler] news_radar error: {e}")


def _model_needs_retrain(db: Session, exchange: str, symbol: str, timeframe: str, horizon_steps: int, policy: dict, df_len: int = 0):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å –ø–æ SLA –ø–æ–ª–∏—Ç–∏–∫–µ
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (need: bool, reason: str, meta: dict)
    """
    from src.db import ModelRun
    
    max_age_days = int(policy.get("max_age_days", 7))
    retrain_if_auc_below = float(policy.get("retrain_if_auc_below", 0.55))
    min_train_rows = int(policy.get("min_train_rows", 200))
    
    # 1) –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    if df_len > 0 and df_len < min_train_rows:
        return False, f"dataset_too_small ({df_len} < {min_train_rows})", {}
    
    # 2) –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –º–æ–¥–µ–ª–∏
    last_run = (
        db.query(ModelRun)
        .filter(
            ModelRun.exchange == exchange,
            ModelRun.symbol == symbol,
            ModelRun.timeframe == timeframe,
            ModelRun.horizon_steps == horizon_steps,
        )
        .order_by(ModelRun.id.desc())
        .first()
    )
    
    if not last_run:
        return True, "no_model", {}
    
    # 3) –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–æ–∑—Ä–∞—Å—Ç–∞ –º–æ–¥–µ–ª–∏
    age_days = (_now_utc().replace(tzinfo=None) - last_run.created_at).days
    if age_days > max_age_days:
        return True, f"model_too_old ({age_days} > {max_age_days} days)", {"age_days": age_days}
    
    # 4) –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
    roc_auc = float(last_run.roc_auc or 0.0)
    if roc_auc < retrain_if_auc_below:
        return True, f"low_auc ({roc_auc:.3f} < {retrain_if_auc_below})", {"roc_auc": roc_auc}
    
    return False, f"model_fresh (age={age_days}d, auc={roc_auc:.3f})", {"age_days": age_days, "roc_auc": roc_auc}


# ============== Scheduler Lock (–¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤) ==============

_SCHED_LOCK_PATH = Path("artifacts/state/scheduler.lock")
_SCHED_LOCK_PATH.parent.mkdir(parents=True, exist_ok=True)
_SCHED_LOCK_OWNED = False


def _pid_alive(pid: int) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∂–∏–≤ –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å —Å –∑–∞–¥–∞–Ω–Ω—ã–º PID"""
    if pid <= 0:
        return False
    try:
        # 1) –ï—Å–ª–∏ –µ—Å—Ç—å psutil ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π –ø—É—Ç—å
        try:
            import psutil  # type: ignore
            return psutil.pid_exists(pid)
        except Exception:
            pass

        # 2) Windows: –ø—Ä–æ–±—É–µ–º –æ—Ç–∫—Ä—ã—Ç—å –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä –ø—Ä–æ—Ü–µ—Å—Å–∞
        if os.name == "nt":
            import ctypes
            PROCESS_QUERY_LIMITED_INFORMATION = 0x1000
            handle = ctypes.windll.kernel32.OpenProcess(PROCESS_QUERY_LIMITED_INFORMATION, 0, pid)
            if handle:
                ctypes.windll.kernel32.CloseHandle(handle)
                return True
            return False

        # 3) POSIX: –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π probe —Å–∏–≥–Ω–∞–ª–æ–º 0
        os.kill(pid, 0)
        return True
    except Exception:
        return False


def _acquire_scheduler_lock() -> bool:
    """
    –ü—ã—Ç–∞–µ–º—Å—è –∞—Ç–æ–º–∞—Ä–Ω–æ —Å–æ–∑–¥–∞—Ç—å lock-—Ñ–∞–π–ª.
    –ï—Å–ª–∏ –æ–Ω –µ—Å—Ç—å ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ–º, –∂–∏–≤ –ª–∏ pid. –ï—Å–ª–∏ –Ω–µ—Ç ‚Äî —á–∏—Å—Ç–∏–º –∏ –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞.
    """
    global _SCHED_LOCK_OWNED
    try:
        fd = os.open(str(_SCHED_LOCK_PATH), os.O_CREAT | os.O_EXCL | os.O_WRONLY)
        os.write(fd, str(os.getpid()).encode("utf-8"))
        os.close(fd)
        _SCHED_LOCK_OWNED = True
        return True
    except FileExistsError:
        try:
            pid = int((_SCHED_LOCK_PATH.read_text() or "0").strip() or "0")
        except Exception:
            pid = 0
        if pid and _pid_alive(pid):
            return False
        try:
            _SCHED_LOCK_PATH.unlink(missing_ok=True)
        except Exception:
            return False
        return _acquire_scheduler_lock()
    except Exception:
        return False


def _release_scheduler_lock():
    """–û—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç scheduler lock"""
    global _SCHED_LOCK_OWNED
    if _SCHED_LOCK_OWNED:
        try:
            _SCHED_LOCK_PATH.unlink(missing_ok=True)
        except Exception:
            pass
        _SCHED_LOCK_OWNED = False


# ============== Startup / Shutdown Events ==============

@app.on_event("startup")
def on_startup():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    if not _acquire_scheduler_lock():
        print("[scheduler] lock is held by another process ‚Äî skipping scheduler init")
        return
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –ë–î
    try:
        from src.db import ensure_runtime_indexes, SessionLocal
        with SessionLocal() as s:
            eng = s.get_bind()
        if eng is not None:
            ensure_runtime_indexes(eng)
            print("[db] indexes ensured")
    except Exception as e:
        print(f"[db] ensure indexes error: {e}")
    
    # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è APScheduler jobs
    scheduler.add_job(
        job_discover_watchlist,
        CronTrigger(hour=0, minute=10),
        id="discover_watchlist",
        replace_existing=True
    )
    scheduler.add_job(
        job_fetch_news,
        IntervalTrigger(minutes=30),
        id="news_fetch",
        replace_existing=True,
        max_instances=1,
        coalesce=True,
        misfire_grace_time=60,
    )
    scheduler.add_job(
        job_analyze_news,
        IntervalTrigger(minutes=30),
        id="news_analyze",
        replace_existing=True,
        max_instances=1,
        coalesce=True,
        next_run_time=_now_utc(),
    )
    scheduler.add_job(
        job_fetch_prices,
        IntervalTrigger(minutes=15),
        id="prices_fetch",
        replace_existing=True
    )
    scheduler.add_job(
        job_train_models,
        CronTrigger(hour=0, minute=35),
        id="train_models",
        replace_existing=True
    )
    scheduler.add_job(
        job_build_report,
        CronTrigger(hour=0, minute=50),
        id="build_report",
        replace_existing=True
    )
    scheduler.add_job(
        job_make_signals,
        IntervalTrigger(minutes=15),
        id="make_signals",
        replace_existing=True
    )
    scheduler.add_job(
        job_resolve_outcomes,
        IntervalTrigger(minutes=5),
        id="resolve_outcomes",
        replace_existing=True
    )
    scheduler.add_job(
        job_monitor_positions,
        IntervalTrigger(minutes=5),
        id="monitor_positions",
        replace_existing=True
    )
    scheduler.add_job(
        job_bootstrap,
        id="bootstrap_once",
        next_run_time=_now_utc(),
        replace_existing=True
    )
    scheduler.add_job(
        job_self_audit_and_notify,
        CronTrigger(hour=1, minute=10),
        id="self_audit",
        replace_existing=True
    )
    scheduler.add_job(
        job_news_radar,
        IntervalTrigger(minutes=10),
        id="news_radar",
        replace_existing=True,
        max_instances=1,
        coalesce=True,
        misfire_grace_time=60,
    )
    
    try:
        scheduler.start()
        print("[scheduler] started (primary)")
    except Exception as e:
        print(f"[scheduler] start error: {e}")
        _release_scheduler_lock()


@app.on_event("shutdown")
def on_shutdown():
    """Cleanup –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    try:
        scheduler.shutdown()
        print("[scheduler] shutdown")
    except Exception:
        pass
    finally:
        _release_scheduler_lock()
    
    # –£—Ç–∏–ª–∏–∑–∞—Ü–∏—è –¥–≤–∏–∂–∫–∞ –ë–î
    try:
        with SessionLocal() as s:
            eng = s.get_bind()
        if eng is not None:
            eng.dispose()
            print("[db] engine disposed")
    except Exception:
        pass


# ============== Boot Log ==============

print("[boot] API_KEY present:", bool(API_KEY))
print("[boot] Offline docs:", USE_OFFLINE)
print("[boot] Docs enabled:", ENABLE_DOCS)
print("[boot] Routers loaded: news, prices, dataset, report, watchlist, risk, notify, models, signals, trade, automation, ui, journal, backup, db_admin, debug")
