# üöÄ PHASE 4: RL-–ü–û–î–•–û–î (PPO Agent)

**–î–∞—Ç–∞:** 2025-10-12 21:15  
**–°—Ç–∞—Ç—É—Å:** –ì–æ—Ç–æ–≤ –∫ –∑–∞–ø—É—Å–∫—É  
**–ü—Ä–µ–¥—ã–¥—É—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:** PHASE 3 –¥–æ—Å—Ç–∏–≥–ª–∞ Test AUC 0.5129 (+6.2%)

---

## ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–´ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –û–®–ò–ë–ö–ò

### 1. Risk Management: Price.dt ‚Üí Price.timestamp
```python
# –ë—ã–ª–æ
.order_by(Price.dt.desc())  # ‚ùå AttributeError

# –°—Ç–∞–ª–æ
.order_by(Price.timestamp.desc())  # ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç
```

**–≠—Ñ—Ñ–µ–∫—Ç:** 68+ –æ—à–∏–±–æ–∫ –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã!

### 2. Paper Trading Monitor: lookback –ø–∞—Ä–∞–º–µ—Ç—Ä —É–±—Ä–∞–Ω
```python
# –ë—ã–ª–æ
df = build_dataset(db, exchange, symbol, timeframe, lookback=200)  # ‚ùå TypeError

# –°—Ç–∞–ª–æ
df, feature_list = build_dataset(db, exchange, symbol, timeframe)  # ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç
```

**–≠—Ñ—Ñ–µ–∫—Ç:** Monitor —Ç–µ–ø–µ—Ä—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏–≥–Ω–∞–ª—ã –±–µ–∑ –æ—à–∏–±–æ–∫!

---

## üìä PHASE 3 –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´

### –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (Run 2)

```
Best Model:     LightGBM
Test AUC:       0.5129 ‚úÖ (>0.5 —Å–ª—É—á–∞–π–Ω–æ–≥–æ!)
Test Accuracy:  50.70%
Improvement:    +6.2% vs PHASE 2

–ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ô –£–°–ü–ï–• –î–û–°–¢–ò–ì–ù–£–¢!
```

### –í—Å–µ –º–æ–¥–µ–ª–∏ (–õ—É—á—à–∏–µ –∏–∑ 2 –∑–∞–ø—É—Å–∫–æ–≤)

| –ú–æ–¥–µ–ª—å | Best AUC | Accuracy |
|--------|----------|----------|
| **LightGBM** | 0.5129 ‚úÖ | 50.70% |
| **Stacking** | 0.4982 | 51.17% |
| **Voting** | 0.4935 | 49.06% |
| **XGBoost** | 0.4931 | 48.59% |
| **CatBoost** | 0.4863 | 48.83% |

---

## üéØ –ü–û–ß–ï–ú–£ –ü–ï–†–ï–•–û–î–ò–ú –ù–ê RL?

**PHASE 3 –ø–æ–∫–∞–∑–∞–ª–∞:** Supervised learning —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ **–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ö–æ—Ä–æ—à–æ**

| –ö—Ä–∏—Ç–µ—Ä–∏–π | PHASE 3 | –¶–µ–ª—å |
|----------|---------|------|
| Test AUC | 0.5129 | >0.60 |
| Sharpe (backtest) | ??? | >1.0 |

**RL –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –õ–£–ß–®–ï:**

1. ‚úÖ **–ù–µ —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–Ω—ã**
   - Supervised: –ù—É–∂–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å UP/DOWN
   - RL: –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç Sharpe Ratio –Ω–∞–ø—Ä—è–º—É—é

2. ‚úÖ **–ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ —Ä—ã–Ω–∫—É**
   - Supervised: –°—Ç–∞—Ç–∏—á–Ω–∞—è –º–æ–¥–µ–ª—å
   - RL: Continuous learning

3. ‚úÖ **–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –ø–æ—Ä—Ç—Ñ–µ–ª—å**
   - Supervised: –¢–æ–ª—å–∫–æ direction
   - RL: Direction + sizing + timing

4. ‚úÖ **–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ì–û–¢–û–í–ê**
   - src/rl_env.py ‚úÖ
   - src/rl_agent.py ‚úÖ
   - src/routers/rl.py ‚úÖ

---

## üöÄ –ó–ê–ü–£–°–ö RL-–û–ë–£–ß–ï–ù–ò–Ø

### –ö–æ–º–∞–Ω–¥–∞

```bash
# –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å venv
.\.venv\Scripts\activate

# –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ (500K timesteps, ~4-6 —á–∞—Å–æ–≤)
python scripts\train_rl_ppo.py --timesteps 500000
```

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

```bash
# –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç (50K timesteps, ~30 –º–∏–Ω—É—Ç)
python scripts\train_rl_ppo.py --timesteps 50000

# –ö–∞—Å—Ç–æ–º–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
python scripts\train_rl_ppo.py \
  --exchange bybit \
  --symbol BTC/USDT \
  --timeframe 1h \
  --timesteps 500000 \
  --initial-capital 1000 \
  --learning-rate 0.0003
```

### Output

```
================================================================================
                    RL-–ü–û–î–•–û–î: PPO Agent Training
================================================================================

Configuration:
  - Exchange: bybit
  - Symbol: BTC/USDT
  - Timeframe: 1h
  - Total Timesteps: 500,000
  - Initial Capital: $1,000.00
  - Learning Rate: 0.0003

Step 1: Loading Dataset
...

Step 3: Training PPO Agent
[*] Starting training (500,000 timesteps)...
     This will take approximately 417-625 minutes (7-10 —á–∞—Å–æ–≤)
     Watch progress: tensorboard --logdir artifacts/tensorboard

[Progress Bar]
...

Step 5: Evaluating Agent

BACKTEST RESULTS
Total Return:    +X.XX%
Sharpe Ratio:    X.XXXX
Max Drawdown:    -X.XX%
Win Rate:        XX.XX%
```

---

## ‚è∞ –û–ñ–ò–î–ê–ï–ú–û–ï –í–†–ï–ú–Ø

| Timesteps | –í—Ä–µ–º—è | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|-----------|-------|--------------|
| **50K** | ~30-60 –º–∏–Ω | –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç |
| **100K** | ~1-2 —á–∞—Å–∞ | –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π |
| **500K** | ~4-6 —á–∞—Å–æ–≤ | **–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è** |
| **1M** | ~8-12 —á–∞—Å–æ–≤ | –î–ª—è production |

**–¢–µ–∫—É—â–∞—è –∫–æ–º–∞–Ω–¥–∞:** 500K timesteps (~4-6 —á–∞—Å–æ–≤)

---

## üìä –ö–†–ò–¢–ï–†–ò–ò –£–°–ü–ï–•–ê

### ‚úÖ –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —É—Å–ø–µ—Ö
- Sharpe Ratio >0.5
- Total Return >0%
- Win Rate >45%

### üéØ –¶–µ–ª–µ–≤–æ–π —É—Å–ø–µ—Ö
- Sharpe Ratio >1.0
- Total Return >5%
- Win Rate >50%
- RL > Supervised Learning (LightGBM)

### üèÜ –ò–¥–µ–∞–ª—å–Ω—ã–π —É—Å–ø–µ—Ö
- Sharpe Ratio >1.5
- Total Return >10%
- Win Rate >55%
- Max Drawdown <10%

---

## üìÅ OUTPUT –§–ê–ô–õ–´

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è:

```
artifacts/
‚îú‚îÄ‚îÄ rl_models/
‚îÇ   ‚îî‚îÄ‚îÄ ppo_btc_usdt_1h_20251012_HHMMSS.zip  # –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
‚îî‚îÄ‚îÄ tensorboard/
    ‚îî‚îÄ‚îÄ PPO_X/  # –õ–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è
```

---

## üí° –ú–û–ù–ò–¢–û–†–ò–ù–ì –û–ë–£–ß–ï–ù–ò–Ø

### –í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è

```bash
# –í –ù–û–í–û–ú —Ç–µ—Ä–º–∏–Ω–∞–ª–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å Tensorboard
tensorboard --logdir artifacts/tensorboard

# –û—Ç–∫—Ä—ã—Ç—å –≤ –±—Ä–∞—É–∑–µ—Ä–µ
http://localhost:6006
```

**–ß—Ç–æ —Å–º–æ—Ç—Ä–µ—Ç—å:**
- `rollout/ep_reward_mean` - —Å—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ (–¥–æ–ª–∂–Ω–∞ —Ä–∞—Å—Ç–∏)
- `train/loss` - loss —Ñ—É–Ω–∫—Ü–∏—è (–¥–æ–ª–∂–Ω–∞ –ø–∞–¥–∞—Ç—å)
- `train/learning_rate` - learning rate

---

## üîß –ï–°–õ–ò –ß–¢–û-–¢–û –ü–û–®–õ–û –ù–ï –¢–ê–ö

### –û—à–∏–±–∫–∞ –ø–∞–º—è—Ç–∏ (MemoryError)

```bash
# –£–º–µ–Ω—å—à–∏—Ç—å batch size
python scripts\train_rl_ppo.py --timesteps 50000
```

### –ú–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å GPU (–µ—Å–ª–∏ –µ—Å—Ç—å)
python -c "import torch; print(torch.cuda.is_available())"

# CPU –Ω–æ—Ä–º–∞–ª—å–Ω–æ, –ø—Ä–æ—Å—Ç–æ –∑–∞–π–º–µ—Ç –¥–æ–ª—å—à–µ
```

### Reward –Ω–µ —Ä–∞—Å—Ç–µ—Ç

–≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ! RL —Ç—Ä–µ–±—É–µ—Ç –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:
- –ü–µ—Ä–≤—ã–µ 10-20% timesteps: exploration (reward –∫–æ–ª–µ–±–ª–µ—Ç—Å—è)
- –°—Ä–µ–¥–Ω–∏–µ 40-60%: exploitation (reward –Ω–∞—á–∏–Ω–∞–µ—Ç —Ä–∞—Å—Ç–∏)
- –ü–æ—Å–ª–µ–¥–Ω–∏–µ 20-30%: convergence (reward —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è)

---

## üéØ –ß–¢–û –î–ï–õ–ê–¢–¨ –ü–û–°–õ–ï –û–ë–£–ß–ï–ù–ò–Ø

### –ï—Å–ª–∏ Sharpe >1.0 ‚úÖ

```bash
1. –ó–∞–ø—É—Å—Ç–∏—Ç—å paper trading —Å RL –º–æ–¥–µ–ª—å—é
2. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ 7 –¥–Ω–µ–π
3. Deploy –≤ production
```

### –ï—Å–ª–∏ Sharpe 0.5-1.0 ‚ö†Ô∏è

```bash
1. –£–≤–µ–ª–∏—á–∏—Ç—å timesteps (500K ‚Üí 1M)
2. Fine-tune hyperparameters
3. Try different reward shaping
```

### –ï—Å–ª–∏ Sharpe <0.5 ‚ùå

```bash
1. –£–≤–µ–ª–∏—á–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç (load more historical data)
2. Try –¥—Ä—É–≥–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã (A2C, SAC)
3. –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –¥—Ä—É–≥–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (buy&hold, mean-reversion)
```

---

## üìö –°–†–ê–í–ù–ï–ù–ò–ï –ü–û–î–•–û–î–û–í

| –ü–æ–¥—Ö–æ–¥ | Test AUC | Sharpe | –°—Ç–∞—Ç—É—Å |
|--------|----------|--------|--------|
| **PHASE 1** | 0.4848 | -1.98 | ‚ùå Baseline |
| **PHASE 2** | 0.4829 (Test) | -1.98 | ‚ùå Overfitting |
| **PHASE 3** | 0.5129 | ??? | ‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω—ã–π —É—Å–ø–µ—Ö |
| **PHASE 4 (RL)** | N/A | ??? | üîÑ –í –ø—Ä–æ—Ü–µ—Å—Å–µ |

---

**–£–¥–∞—á–∏ —Å RL-–æ–±—É—á–µ–Ω–∏–µ–º! üöÄ**

**–û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è:** ~4-6 —á–∞—Å–æ–≤  
**Tensorboard:** http://localhost:6006

