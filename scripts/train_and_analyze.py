"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞ feature importance
–ó–∞–ø—É—Å–∫: python scripts/train_and_analyze.py
"""
import os
import sys
from pathlib import Path
import json
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –±–µ–∑ GUI
import matplotlib.pyplot as plt

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.db import get_db, init_db
from src.features import build_dataset
from src.modeling import train_xgb_and_save, load_latest_model
import joblib


def analyze_feature_importance(model_path: str, feature_cols: list, top_n: int = 20):
    """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è"""
    print(f"\n{'='*70}")
    print(f"üìä FEATURE IMPORTANCE –ê–ù–ê–õ–ò–ó")
    print(f"{'='*70}\n")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    obj = joblib.load(model_path)
    model = obj.get("model") if isinstance(obj, dict) else obj
    
    if not hasattr(model, "feature_importances_"):
        print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç feature_importances_")
        return
    
    # –ü–æ–ª—É—á–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å —Ñ–∏—á
    importance = model.feature_importances_
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': importance
    }).sort_values('importance', ascending=False)
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ñ–∏—á
    def categorize_feature(feat):
        if feat.startswith('ret_') or feat == 'vol_norm':
            return 'Price'
        elif feat.startswith('rsi') or feat.startswith('bb_') or feat.startswith('macd') or \
             feat.startswith('atr') or feat.startswith('adx') or feat.startswith('stoch') or \
             feat.startswith('williams') or feat.startswith('cci') or feat.startswith('ema'):
            return 'Technical'
        elif feat.startswith('news_') or feat.startswith('sent_') or feat.startswith('tag_'):
            return 'News'
        elif feat.startswith('onchain_'):
            return 'OnChain'
        elif feat.startswith('macro_'):
            return 'Macro'
        elif feat.startswith('social_'):
            return 'Social'
        else:
            return 'Other'
    
    feature_importance['category'] = feature_importance['feature'].apply(categorize_feature)
    
    # –¢–æ–ø-20 —Ñ–∏—á
    print(f"üèÜ –¢–û–ü-{top_n} –í–ê–ñ–ù–´–• –§–ò–ß:\n")
    print(feature_importance.head(top_n).to_string(index=False))
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    print(f"\n\nüìà –í–ê–ñ–ù–û–°–¢–¨ –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:\n")
    category_stats = feature_importance.groupby('category').agg({
        'importance': ['sum', 'mean', 'count']
    }).round(4)
    category_stats.columns = ['Total_Importance', 'Avg_Importance', 'Count']
    category_stats = category_stats.sort_values('Total_Importance', ascending=False)
    print(category_stats)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_dir = Path("artifacts/analysis")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # JSON –æ—Ç—á—ë—Ç
    report = {
        "top_features": feature_importance.head(top_n).to_dict('records'),
        "category_stats": category_stats.reset_index().to_dict('records'),
        "total_features": len(feature_cols),
        "timestamp": pd.Timestamp.now().isoformat()
    }
    
    with open(output_dir / "feature_importance.json", "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # –ì—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ç–æ–ø-20
    plt.figure(figsize=(12, 8))
    top_features = feature_importance.head(top_n)
    colors = top_features['category'].map({
        'Price': '#FF6B6B',
        'Technical': '#4ECDC4',
        'News': '#45B7D1',
        'OnChain': '#FFA07A',
        'Macro': '#98D8C8',
        'Social': '#F7DC6F'
    })
    
    plt.barh(range(top_n), top_features['importance'], color=colors)
    plt.yticks(range(top_n), top_features['feature'])
    plt.xlabel('Importance', fontsize=12)
    plt.title(f'Top {top_n} Feature Importance', fontsize=14, fontweight='bold')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.savefig(output_dir / "feature_importance_top20.png", dpi=150, bbox_inches='tight')
    plt.close()
    
    # –ì—Ä–∞—Ñ–∏–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    plt.figure(figsize=(10, 6))
    category_stats.plot(kind='bar', y='Total_Importance', legend=False, color='#4ECDC4')
    plt.xlabel('Category', fontsize=12)
    plt.ylabel('Total Importance', fontsize=12)
    plt.title('Feature Importance by Category', fontsize=14, fontweight='bold')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(output_dir / "feature_importance_by_category.png", dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
    print(f"   - {output_dir / 'feature_importance.json'}")
    print(f"   - {output_dir / 'feature_importance_top20.png'}")
    print(f"   - {output_dir / 'feature_importance_by_category.png'}\n")
    
    return feature_importance


def compare_with_baseline(new_metrics: dict, baseline_path: str = "artifacts/metrics.json"):
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å baseline"""
    print(f"\n{'='*70}")
    print(f"üìä –°–†–ê–í–ù–ï–ù–ò–ï –° BASELINE")
    print(f"{'='*70}\n")
    
    try:
        with open(baseline_path, "r") as f:
            baseline = json.load(f)
        
        print(f"{'–ú–µ—Ç—Ä–∏–∫–∞':<20} {'Baseline':<15} {'New Model':<15} {'–ò–∑–º–µ–Ω–µ–Ω–∏–µ':<15}")
        print("-" * 70)
        
        metrics_to_compare = ['accuracy', 'roc_auc', 'sharpe_like', 'total_return']
        improvements = {}
        
        for metric in metrics_to_compare:
            base_val = baseline.get(metric)
            new_val = new_metrics.get(metric)
            
            if base_val is not None and new_val is not None:
                if base_val != 0:
                    change_pct = ((new_val - base_val) / abs(base_val)) * 100
                else:
                    change_pct = 0
                
                change_str = f"{change_pct:+.1f}%"
                if change_pct > 0:
                    change_str = f"‚úÖ {change_str}"
                elif change_pct < 0:
                    change_str = f"‚ùå {change_str}"
                else:
                    change_str = "‚Üí 0.0%"
                
                improvements[metric] = change_pct
                
                print(f"{metric:<20} {base_val:<15.4f} {new_val:<15.4f} {change_str:<15}")
            else:
                print(f"{metric:<20} {'N/A':<15} {'N/A':<15} {'N/A':<15}")
        
        # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        print("\n" + "="*70)
        avg_improvement = sum(improvements.values()) / len(improvements) if improvements else 0
        
        if avg_improvement > 5:
            verdict = "üéâ –û–¢–õ–ò–ß–ù–û–ï –£–õ–£–ß–®–ï–ù–ò–ï!"
        elif avg_improvement > 0:
            verdict = "‚úÖ –£–ª—É—á—à–µ–Ω–∏–µ"
        elif avg_improvement > -5:
            verdict = "‚ö†Ô∏è –ù–µ–±–æ–ª—å—à–æ–µ —É—Ö—É–¥—à–µ–Ω–∏–µ"
        else:
            verdict = "‚ùå –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Ö—É–¥—à–µ–Ω–∏–µ"
        
        print(f"–°—Ä–µ–¥–Ω—è—è —Ä–∞–∑–Ω–∏—Ü–∞: {avg_improvement:+.1f}%")
        print(f"–í–µ—Ä–¥–∏–∫—Ç: {verdict}")
        print("="*70 + "\n")
        
        return improvements
        
    except FileNotFoundError:
        print("‚ùå Baseline –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å —Å—Ç–∞–Ω–µ—Ç baseline.\n")
        return None


def main():
    print("\n" + "="*70)
    print("üöÄ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ò FEATURE IMPORTANCE –ê–ù–ê–õ–ò–ó")
    print("="*70 + "\n")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    EXCHANGE = "bybit"
    SYMBOL = "BTC/USDT"
    TIMEFRAME = "1h"
    HORIZON_STEPS = 6
    
    print(f"üìå –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"   Exchange: {EXCHANGE}")
    print(f"   Symbol: {SYMBOL}")
    print(f"   Timeframe: {TIMEFRAME}")
    print(f"   Horizon: {HORIZON_STEPS} steps\n")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
    print("üîÑ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î...")
    init_db()
    db = next(get_db())
    
    try:
        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
        print("üîÑ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ —Ñ–∏—á–∞–º–∏...")
        df, feature_cols = build_dataset(db, EXCHANGE, SYMBOL, TIMEFRAME, HORIZON_STEPS)
        print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –ø–æ—Å—Ç—Ä–æ–µ–Ω: {len(df)} —Å—Ç—Ä–æ–∫ √ó {len(feature_cols)} —Ñ–∏—á\n")
        
        if len(df) < 200:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (< 200 —Å—Ç—Ä–æ–∫)")
            return
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ä—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞–∫ baseline (–µ—Å–ª–∏ –µ—Å—Ç—å)
        baseline_path = Path("artifacts/metrics.json")
        if baseline_path.exists():
            baseline_backup = Path("artifacts/metrics_baseline.json")
            import shutil
            shutil.copy(baseline_path, baseline_backup)
            print(f"üíæ Baseline —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {baseline_backup}\n")
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("üîÑ –û–±—É—á–µ–Ω–∏–µ XGBoost –º–æ–¥–µ–ª–∏...")
        print("   (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 2-5 –º–∏–Ω—É—Ç –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö)\n")
        
        metrics, model_path = train_xgb_and_save(
            df, 
            feature_cols, 
            artifacts_dir="artifacts",
            mlflow_experiment="myassistent-trading",
            mlflow_run_name=f"{SYMBOL}_{TIMEFRAME}_training"
        )
        
        print(f"\n‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
        print(f"\nüìä –ú–ï–¢–†–ò–ö–ò –ú–û–î–ï–õ–ò:")
        print(f"   - Accuracy: {metrics['accuracy']:.4f}")
        print(f"   - ROC AUC: {metrics.get('roc_auc', 0):.4f}")
        print(f"   - Threshold: {metrics['threshold']:.4f}")
        print(f"   - Total Return: {metrics['total_return']:.4f} ({metrics['total_return']*100:.2f}%)")
        print(f"   - Sharpe-like: {metrics.get('sharpe_like', 0):.4f}")
        print(f"   - Train size: {metrics['n_train']} rows")
        print(f"   - Test size: {metrics['n_test']} rows\n")
        
        # Feature Importance –ê–Ω–∞–ª–∏–∑
        analyze_feature_importance(model_path, feature_cols, top_n=20)
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å baseline
        if Path("artifacts/metrics_baseline.json").exists():
            compare_with_baseline(metrics, "artifacts/metrics_baseline.json")
        else:
            print("‚ÑπÔ∏è Baseline –º–µ—Ç—Ä–∏–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç. –¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å - –ø–µ—Ä–≤–∞—è.\n")
        
        print("="*70)
        print("‚úÖ –í–°–Å –ì–û–¢–û–í–û!")
        print("="*70)
        print("\n–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
        print("1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ MLflow UI: http://localhost:5000")
        print("2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –±—ç–∫—Ç–µ—Å—Ç: POST /backtest/run")
        print("3. –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –≥—Ä–∞—Ñ–∏–∫–∏ –≤ artifacts/analysis/\n")
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
    finally:
        db.close()


if __name__ == "__main__":
    main()

