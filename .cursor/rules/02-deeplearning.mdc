---
alwaysApply: false
---

# Rule: Deep Learning (PyTorch, Transformers, Diffusers)

**Scope:** training/fine-tuning LLMs/transformers, diffusion models, and custom NN modules.  
**Outcome:** reproducible training runs with tracked metrics, checkpoints, and inference demos.

## Key Principles
- PyTorch for models; OOP for `nn.Module`, functional for data pipelines.
- Mixed precision when possible; correct device placement and gradient checks.
- Prefer efficient finetuning (LoRA/PEFT) before full training.

## Toolkit
- `torch`, `transformers`, `diffusers`, `accelerate`, `datasets`, `wandb|tensorboard`, `gradio`.

## Training
- Proper splits; early stopping; LR schedules; gradient clipping; NaN guards.
- Use `DataLoader` with workers/pinning; log losses + task metrics per epoch.
- Save/validate checkpoints; keep config (YAML) for hparams.

## Inference & Demos
- Ship a minimal `inference.py` and a `gradio` demo with input validation.

## Do
- Use `DistributedDataParallel` for multi-GPU; accumulate grads for big batches.

## Don’t
- Commit large checkpoints; store via artifact storage.
# Rule: Deep Learning (PyTorch, Transformers, Diffusers)

**Scope:** training/fine-tuning LLMs/transformers, diffusion models, and custom NN modules.  
**Outcome:** reproducible training runs with tracked metrics, checkpoints, and inference demos.

## Key Principles
- PyTorch for models; OOP for `nn.Module`, functional for data pipelines.
- Mixed precision when possible; correct device placement and gradient checks.
- Prefer efficient finetuning (LoRA/PEFT) before full training.

## Toolkit
- `torch`, `transformers`, `diffusers`, `accelerate`, `datasets`, `wandb|tensorboard`, `gradio`.

## Training
- Proper splits; early stopping; LR schedules; gradient clipping; NaN guards.
- Use `DataLoader` with workers/pinning; log losses + task metrics per epoch.
- Save/validate checkpoints; keep config (YAML) for hparams.

## Inference & Demos
- Ship a minimal `inference.py` and a `gradio` demo with input validation.

## Do
- Use `DistributedDataParallel` for multi-GPU; accumulate grads for big batches.

## Don’t
- Commit large checkpoints; store via artifact storage.
